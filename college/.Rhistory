abs_err_lassoreg <- abs(pred_lassoreg - test$Apps)
models_comp <- rbind(models_comp, "LASSOReg" = c(rmse(test$Apps, pred_lassoreg),
mean(abs_err_lassoreg),
median(abs_err_lassoreg),
sd(abs_err_lassoreg),
IQR(abs_err_lassoreg),
range(abs_err_lassoreg)))
#Actual vs. Predicted
plot(test$Apps, pred_lassoreg, main = 'LASSOReg',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
knitr::kable(
models_comp , caption = 'Comparing models.'
)
tree_1 <- rpart(Apps ~ Private + Accept + Enroll + Top10perc + F.Undergrad
+ Outstate + Room.Board + Expend  + PhD, data = train2,
control = list(cp = 0.01, maxdepth = 10, minbucket = 20))
tree_2 <- rpart(Apps ~ Private + Accept + Enroll + Top10perc + F.Undergrad
+ Outstate + Room.Board + Expend  + PhD, data = train2,
control = list(cp = 0.001, maxdepth = 20, minbucket = 20))
tree_2P <- prune.rpart(tree_2,
cp = tree_2$cptable[which.min(tree_2$cptable[,"xerror"])])
prp(tree_2P)
tree_3 <- rpart(Apps ~ ., data = train2,
control = list(cp = 0.0001, maxdepth = 20, minbucket = 5))
tree_3P <- prune.rpart(tree_2,
cp = tree_2$cptable[which.min(tree_2$cptable[,"xerror"])])
#Plot the pruned tree
prp(tree_3P)
pred_tree  <- predict(tree_3P, test)
## Absolute error mean, median, sd, max, min
abs_err_tree <- abs(pred_tree - test$Apps)
models_comp  <- rbind(models_comp, "TreeReg" = c(rmse(test$Apps, pred_tree),
mean(abs_err_tree),
median(abs_err_tree),
sd(abs_err_tree),
IQR(abs_err_tree),
range(abs_err_tree)))
## Actual vs. Predicted
plot(test$Apps, pred_tree, main = 'TreeReg',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
View(models_comp)
bagging <- randomForest(Apps ~ . , mtry = ncol(train) - 2, ntree = 500, data = train2)
bagging
## Absolute error mean, median, sd, max, min
pred_bagging  <- predict(bagging, test)
abs_err_bagging <- abs(pred_bagging - test$Apps)
models_comp <- rbind(models_comp, "Bagging2" = c(rmse(test$Apps, pred_bagging),
mean(abs_err_bagging),
median(abs_err_bagging),
sd(abs_err_bagging),
IQR(abs_err_bagging),
range(abs_err_bagging)))
## Actual vs. Predicted
plot(test$Apps, pred_bagging, main = 'Bagging',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
View(models_comp)
rf_1 <- randomForest(Apps ~ . , data = train,
mtry = 6, ntree = 500, nodesize = 5, importance = TRUE)
rf_1
varImpPlot(rf_1)
varImpPlot(rf_1)
rf_cv <- rfcv(train[, - c(2)],
train$Apps,
cv.fold = 10,
step = 0.75,
mtry = function(p) max(1, floor(sqrt(p))),
recursive = FALSE)
str(rf_cv)
## Regression formula
reg_formula <- as.formula(Apps ~ Accept  + Enroll + F.Undergrad + Top10perc +
Room.Board  + Grad.Rate + Top25perc + Outstate +
Expend + Private + perc.alumni + Terminal + PhD +
P.Undergrad )
rf_2 <- randomForest(reg_formula, data = train2, mtry = 3, ntree = 500, nodesize = 5)
rf_2
View(models_comp)
pred_rf  <- predict(rf_2, test)
abs_err_rf <- abs(pred_rf - test$Apps)
models_comp <- rbind(models_comp, "RandomForest_model9" = c(rmse(test$Apps, pred_rf),
mean(abs_err_rf),
median(abs_err_rf),
sd(abs_err_rf),
IQR(abs_err_rf),
range(abs_err_rf)))
plot(test$Apps, pred_rf, main = 'RandomForest_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
View(models_comp)
knitr::kable(
models_comp , caption = 'Comparing models.'
)
par_grid <- expand.grid(shrinkage = c(0.01, 0.15, 0.1, 0.3),  #learning rate
interaction_depth = c(1,2 ,3, 5), #the maximum depth of each tree
n_minobsinnode = c(5, 10, 15, 20),  #the minimum number of observations in the terminal nodes of the trees
bag_fraction = c(0.5, 0.7, 0.8,0.9) #stochastic gradient :bag.fraction < 1
)
## Grid search (train/validation approach)
for(i in 1:nrow(par_grid)) {
set.seed(123)
#train model
gbm_tune <- gbm(formula =  Apps ~ + .,
distribution = "gaussian",
data = train,
n.trees = 5000,
interaction.depth = par_grid$interaction_depth[i],
shrinkage = par_grid$shrinkage[i],
n.minobsinnode = par_grid$n_minobsinnode[i],
bag.fraction = par_grid$bag_fraction[i],
train.fraction = 0.8,
#cv.folds = 5,
n.cores = NULL, #will use all cores by default
verbose = FALSE)
#add min training error and trees to grid
par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
par_grid$min_RMSE[i]      <- sqrt(min(gbm_tune$valid.error))
}
##load("case1_testPreproces.R")
summary(train2)
##load("case1_testPreproces.R")
summary(data)
train2$Private <- factor(train2$Private)
summary(train2)
summary(test)
knitr::opts_chunk$set(echo = TRUE)
library("moments")
library("ggplot2")
library("MASS")
library("leaps")
library("corrplot")
library("carData")
library("car")
library("corrplot")
library("glmnet")
library("randomForest")
library("leaps")
library("rpart")
library("rpart.plot")
library("gbm")
library("xgboost")
library("Metrics")
dataRaw<- read.csv("../college.csv", header = TRUE)
print(colnames(dataRaw))
str(dataRaw)
#Remove College.Name
length(unique(dataRaw$College.Name))
data <- dataRaw[,-1]
##Uni variate Profiling
length(unique(data$Private))
unique(data$Private)
data$Private <- factor(data$Private)
summary(data)
data <- dataRaw[,-1]
#Categorical variables
table(data$Private)
#distribution
par(mfrow = c(3, 3))  # 4 rows and 4 columns
#Continuous variables distribution
for (i in 3:18) {
hist(data[, i], xlab = "",  probability = T, breaks = 15,
main = paste("Histogram of", names(data)[i]))
lines(density(data[,i]), col = "red")
}
par(mfrow = c(3, 3))
for (i in 3:18) {
qqnorm(data[,i], main = paste("QQ plot of", names(data)[i]), pch = 20)
qqline(data[,i], col = "red")
}
#Scatter Plot
par(mfrow = c(2, 2))  # 2 rows and 2 columns
for (i in 3:18) {
plot(data[,i], data$Apps, xlab = "", main = paste("Apps vs.", names(data)[i]))
}
cor_table <- round(cor(data[, c(2: 18)]), 2)
corrplot(cor_table)
boxplot(data$Apps, main = "Number of applications received")
tukey_ul <- quantile(data$Apps, probs = 0.75) + 1.5 * IQR(data$Apps)
tukey_ul
sum(data$Apps > tukey_ul)
sum(data$Apps > tukey_ul)/nrow(data) * 100
summary(train)
train2
train2$Private <- factor(train2$Private)
train2
test
train2$Private <- factor(train2$Private)
test$Private <- factor(test$Private)
par_grid <- expand.grid(shrinkage = c(0.01, 0.15, 0.1, 0.3),  #learning rate
interaction_depth = c(1,2 ,3, 5), #the maximum depth of each tree
n_minobsinnode = c(5, 10, 15, 20),  #the minimum number of observations in the terminal nodes of the trees
bag_fraction = c(0.5, 0.7, 0.8,0.9) #stochastic gradient :bag.fraction < 1
)
## Grid search (train/validation approach)
for(i in 1:nrow(par_grid)) {
set.seed(123)
#train model
gbm_tune <- gbm(formula =  Apps ~ + .,
distribution = "gaussian",
data = train2,
n.trees = 5000,
interaction.depth = par_grid$interaction_depth[i],
shrinkage = par_grid$shrinkage[i],
n.minobsinnode = par_grid$n_minobsinnode[i],
bag.fraction = par_grid$bag_fraction[i],
train.fraction = 0.8,
#cv.folds = 5,
n.cores = NULL, #will use all cores by default
verbose = FALSE)
#add min training error and trees to grid
par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
par_grid$min_RMSE[i]      <- sqrt(min(gbm_tune$valid.error))
}
View(models_comp)
View(models_comp)
View(models_comp)
View(models_comp)
View(models_comp)
head(par_grid)
head(par_grid)
gbmFinal <- gbm(formula = Apps  ~ . ,
distribution = "gaussian",
data = train2,
n.trees = 1622,
interaction.depth = 2,
shrinkage =0.01,
n.minobsinnode = 5,
bag.fraction = 0.6,
train.fraction = 1,
cv.folds = 0,
n.cores = NULL, #will use all cores by default
)
summary(gbmFinal)
pred_gbm <- predict(gbmFinal, n.trees = 1622, newdata = test)
## Absolute error mean, median, sd, max, min
abs_err_gbm <- abs(pred_gbm - test$Apps)
models_comp <- rbind(models_comp, "GBReg_model9" = c(rmse(test$Apps, pred_gbm),
mean(abs_err_gbm),
median(abs_err_gbm),
sd(abs_err_gbm),
IQR(abs_err_gbm),
range(abs_err_gbm)))
## Actual vs. Predicted
plot(test$Apps, pred_gbm, main = 'GBReg_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
View(models_comp)
set.seed(123)
xgb <- xgboost(data = x,
label = y,
eta = 0.1,     #learning rate
max_depth = 3,  #tree depth
lambda = 0,
nround = 1000,
colsample_bytree = 0.9,
subsample = 0.65,                #percent of training data to sample for each tree
objective = "reg:squarederror",  #for regression models
verbose = 0,                      #silent
early_stopping_rounds = 10
)
x_test   <- model.matrix(Apps ~ . , data = test)[, -1]#remove intercept
pred_xgb <- predict(xgb, x_test)
## Absolute error mean, median, sd, max, min
abs_err_xgb <- abs(pred_xgb - test$Apps)
models_comp <- rbind(models_comp, "XGBReg_model10" = c(rmse(test$Apps, pred_xgb),
mean(abs_err_xgb),
median(abs_err_xgb),
sd(abs_err_xgb),
IQR(abs_err_xgb),
range(abs_err_xgb)))
## Actual vs. Predicted
plot(test$Apps, pred_xgb, main = 'XGBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
View(models_comp)
knitr::kable(
models_comp , caption = 'Comparing models.'
)
# par_grid2 <- expand.grid(shrinkage = c(0.01, 0.15, 0.1, 0.3),  #learning rate
#                         interaction_depth = c(2,3), #the maximum depth of each tree
#                         n_minobsinnode = c(5) , #the minimum number of observations in the terminal nodes of the trees
#                         bag_fraction = c(0.6, 0.7,0.8, 0.9)
#                         )
# ## Grid search (train/validation approach)
# for(i in 1:nrow(par_grid2)) {
#   set.seed(123)
#   #train model
#   gbm_tune <- gbm(formula = Apps ~ + .,
#                   distribution = "gaussian",
#                   data = train2,
#                   n.trees = 5000,
#                   interaction.depth = par_grid2$interaction_depth[i],
#                   shrinkage = par_grid2$shrinkage[i],
#                   n.minobsinnode = par_grid2$n_minobsinnode[i],
#                   bag.fraction = par_grid2$bag_fraction[i],
#                   train.fraction = 0.8,
#                   cv.folds = 0,
#                   n.cores = NULL, #will use all cores by default
#                   verbose = FALSE)
#   #add min training error and trees to grid
#   par_grid2$optimal_trees[i] <- which.min(gbm_tune$valid.error)
#   par_grid2$min_RMSE[i]    <- sqrt(min(gbm_tune$valid.error))
# }
# par_grid <- expand.grid(shrinkage = c(0.01, 0.15, 0.1, 0.3),  #learning rate
#                         interaction_depth = c(1,2 ,3, 5), #the maximum depth of each tree
#                         n_minobsinnode = c(5, 10, 15, 20),  #the minimum number of observations in the terminal nodes of the trees
#                         bag_fraction = c(0.5, 0.7, 0.8,0.9) #stochastic gradient :bag.fraction < 1
# )
#
#
# ## Grid search (train/validation approach)
# for(i in 1:nrow(par_grid)) {
#   set.seed(123)
#   #train model
#   gbm_tune <- gbm(formula =  Apps ~ + .,
#                   distribution = "gaussian",
#                   data = train2,
#                   n.trees = 5000,
#                   interaction.depth = par_grid$interaction_depth[i],
#                   shrinkage = par_grid$shrinkage[i],
#                   n.minobsinnode = par_grid$n_minobsinnode[i],
#                   bag.fraction = par_grid$bag_fraction[i],
#                   train.fraction = 0.8,
#                   #cv.folds = 5,
#                   n.cores = NULL, #will use all cores by default
#                   verbose = FALSE)
#   #add min training error and trees to grid
#   par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
#   par_grid$min_RMSE[i]      <- sqrt(min(gbm_tune$valid.error))
# }
# #
# head(par_grid)
plot(test$Apps, pred_xgb, main = 'XGBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mAll, main = 'LM_AllFeature_model1',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mTtest, main = 'LM_Ttest_model2',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsubAdj, main = 'BestSubset_RAdj_model3',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsub_cv, main = 'BestSubset_CV_model4',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_ridgereg, main = 'RidgeReg_model5',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_lassoreg, main = 'LASSOReg_model6',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_tree, main = 'TreeReg_model7',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bagging, main = 'Bagging_model8',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_rf, main = 'RandomForest_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_xgb, main = 'XGBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mAll, main = 'LM_AllFeature_model1',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mTtest, main = 'LM_Ttest_model2',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsubAdj, main = 'BestSubset_RAdj_model3',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsub_cv, main = 'BestSubset_CV_model4',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_ridgereg, main = 'RidgeReg_model5',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_lassoreg, main = 'LASSOReg_model6',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_tree, main = 'TreeReg_model7',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bagging, main = 'Bagging_model8',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_rf, main = 'RandomForest_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_xgb, main = 'XGBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
2022-1995
plot(test$Apps, pred_mAll, main = 'LM_AllFeature_model1',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mTtest, main = 'LM_Ttest_model2',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsubAdj, main = 'BestSubset_RAdj_model3',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsub_cv, main = 'BestSubset_CV_model4',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_ridgereg, main = 'RidgeReg_model5',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_lassoreg, main = 'LASSOReg_model6',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_tree, main = 'TreeReg_model7',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bagging, main = 'Bagging_model8',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_rf, main = 'RandomForest_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_gbm, main = 'GBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_xgb, main = 'XGBReg_model11',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mAll, main = 'LM_AllFeature_model1',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_mTtest, main = 'LM_Ttest_model2',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsubAdj, main = 'BestSubset_RAdj_model3',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bestsub_cv, main = 'BestSubset_CV_model4',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_ridgereg, main = 'RidgeReg_model5',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_lassoreg, main = 'LASSOReg_model6',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_tree, main = 'TreeReg_model7',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_bagging, main = 'Bagging_model8',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_rf, main = 'RandomForest_model9',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_gbm, main = 'GBReg_model10',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
plot(test$Apps, pred_xgb, main = 'XGBReg_model11',
xlim = c(0, 2000), ylim = c(0, 2000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
